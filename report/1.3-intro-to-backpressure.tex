\section{Fast producers, slow consumers}
\label{sec:fastproc-slowcons}
In the previous section we discussed that a reactive collection is equivalent to any interactive collection: they obey to the same rules and the same operators (like \code{map} and \code{filter}) can be defined. The only difference is that a reactive collection is push-based, whereas an interactive collection is pull-based. Rather than the consumer being in charge, asking for the next value, here the producer is in charge and \emph{it} decides when to emit the next value. The consumer just has to listen and can only react to the elements emitted by the producer. This is conform the definition of a reactive collection in \Cref{sec:reactive-programming}: ``\textit{Reactive programs maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not by the program itself.}''.

A risk that arises from allowing the producer to be in charge occurs when the consumer cannot keep up with the rate in which the producer is producing the data. This gives rise to the problem of what to do with the growing accumulation of unconsumed data.

A classic example of overproducing streams is the \code{zip} operator, which merges two (or more) streams by using a combiner function whenever both streams have produced an element. In this operator the problem occurs when one \obs always produces faster than the other. This is a problem, because \code{zip} cannot keep up with the rate in which the first \obs is producing, since the second \obs is much slower. The most common, but also slightly naive, implementation for this operator maintains an ever expending buffer of elements emitted by the first \obs to eventually combine them with the data emitted by the slower stream.

Buffering the unprocessed elements of a stream is a common answer to this problem. A fast emitting stream is draining its data into a buffer and the much slower \obv eventually takes the data out of the buffer. This is also what happens in \Cref{lst:observeOn} with the \code{observeOn} operator. If the first two lines produce data in a faster pace than the last two lines can consume, the elements will be buffered inside the \code{observeOn}. The major problem with this buffering solution (and thus with the implementation of \code{zip} as described above) is that the buffer will have to use a potentially unwieldy amount of system resources.

In this section we will explore some alternatives of dealing with fast producers and slow consumers.

\subsection{Avoiding overproduction}
\label{subsec:avoiding-overproduction}
One way of dealing with overproducing streams is to simply avoid the problem and take proactive measures whenever this is expected to happen. In the following we will discuss several ways to reduce the amount of data by using standard operators that are defined on the \obs interface. For this we distinguish two types of operators: lossy and lossless operators \cite{Backpressure-Explained}.

\subsubsection{Lossy operators}
One set of operators avoids the problem in a lossy way, meaning that the data that cannot be consumed immediately will be dropped.

\paragraph{Throttle} The \code{throttle(interval: Duration)} operator only propagates the first element that is received in a particular interval. All other elements are discarded. Once an interval has finished, a new interval starts immediately, in which again the first received element is propagated and all other elements are discarded.

\paragraph{Sample} Rather than propagating the first element and discarding all others, the \code{sample(interval: Duration)} operator is used to discard all elements except for the last one that is received in a certain \code{interval}. This operator can for example be used when someone only wants to receive the newest data from a stock ticker, but only every 5 seconds, without the need to process every value that comes in between.

\paragraph{Debounce} The operator \code{debounce(timespan: Duration)} will only propagate its received values after a certain \code{timespan} has passed without receiving any other values. When a value is received within the \code{timespan}, the previous value is discarded and the same process starts all over again with the newly received value. This operator is most commonly used in text fields within user interfaces, in order to avoid too much keystroke events being generated by a fast typing user. Rather than every keystroke being emitted, this operator will only yield the last keystroke after a particular \code{timespan}. Notice that in some versions of Rx this operator is also referred to as \code{throttleWithTimeout}.

With these operators many situations of potentially overproducing streams can be avoided by eliminating all the elements that do not really matter for a particular application. Note that, since these operators are depending on intervals, they have to operate on a \sch, which is supplied as an extra argument to either one of these operators.

\subsubsection{Loss-less operators}
Even though these lossy operators solve a large part of the problem, there are still as many cases left in which \emph{all} data that is send over a stream needs to be processed. For these kinds of use cases Rx provides so-called loss-less operators. These operators can buffer or group the data and emit collections of data, such that there are at least less elements to deal with. Note that this is different from buffering as discussed at the start of this section, as loss-less operators group elements and emits them \emph{all at once} in a collection rather than storing them as a backlog to be consumed \emph{one by one} at a later moment in time!

\paragraph{Buffer} The \code{buffer} operator comes in a couple of different overloads. First of all, the \code{buffer(n: Int): Observable[List[T]]} operator, which receives data and groups it into lists of \code{n} elements. Once a list is filled (e.g. when its size is \code{n}), it is emitted to downstream. Until then, the operator holds the data it receives. The \code{buffer} operator also comes in another form: \code{buffer(interval: Duration)}, which groups the data it receives within a certain interval into a single list. Just as with the lossy operators, note that this operator depends on an interval and therefore requires a \sch. Finally there is \code{buffer[B](boundary: Observable[B])}, which groups the data it receives between two emissions of \code{boundary} into a single list.

\paragraph{Window} The drawback of a buffer is that it only propagates its received values once the buffer is filled, the boundary \obs fires or the interval is over. All the time in between, no elements will be received by the observer or downstream operators. This already becomes clear from the return type: \code{Observable[List[T]]}. In order to accomplish the same behavior but with an \obs rather than a \code{List}, the \code{window} operator is included in Rx, having the same kinds of overloads as \code{buffer}. Instead of the list being emitted only once it is completely filled, the inner \obs is emitted as soon as the first element is received and is \emph{completed} once the size, interval or boundary requirement is met.

These operators form a first line of defense against overproduction. For streams where not all elements necessarily need to be processed (for instance the keyboard events on a text field), a lossy operator can be used. For streams where all emitted data is needed, a loss-less operator is the right solution. Besides that, for special occasions lossy and loss-less backpressure operators can be combined in order to create the optimal buffering strategy. This was discussed in some further detail in a conference talk at QCon by Ben Christensen \cite{christensen2014-RxServiceArchitecture}.

\subsection{Callstack blocking}
\label{subsec:callstack-blocking}
Another way of dealing with this problem is to block the callstack and with that `park' the thread on which the \obs is running. This directly slows down the producer and gives the consumer more time to process each element of the stream. Despite the fact that this approach goes against the `reactive' and `non-blocking' model of Rx, it can potentially be a viable option if the overproducing \obs runs on a thread that can safely be blocked.

This technique is currently not used in RxJava \cite{RxJava-Wiki-Callstack-Blocking}, but \emph{is} used in a particular implementation of the \code{zip} operator in RxMobile \cite{RxMobile}. Once either one of the streams emits a value, it is blocked until the other \obs has produced a value as well. In order to avoid blocking the upstream callstack completely, it is strongly recommended to switch each \obs to another thread or scheduler using the \code{observeOn} operator. With this only the callstack is blocked upto the start of this new thread or scheduler. Once the other stream has produced a value, the blocking of the first \obs sequence is removed and the \code{zip} operator waits until either one of the sources emits a next value.

It is debatable whether or not this is a really effective approach. The elements are not buffered in the \code{zip} operator now, but are still buffered in the \code{observeOn}.

\subsection{Reactive Streams}
\label{subsec:reactive-streams}
Reactive Streams is an initiative \cite{Reactive-Streams} of a number of companies such as Netflix, Pivotal and Lightbend with the mission to \textit{provide a standard for asynchronous stream processing with non-blocking backpressure}. This collaboration resulted in an alternative API \cite{Reactive-Streams-API} for stream processing that is claimed to be capable of handling overproducing streams using backpressure.

The Reactive Streams API (see \Cref{lst:pub-sub}) looks fairly similar at first glance to the original API that was developed by Microsoft, but has some particular differences that have significant influence on the handling of backpressure. The \obs, renamed \code{Publisher}, looks the same: it still is parameterized over \code{T} and still has a \code{subscribe} method. The \obv, which was passed to the \code{subscribe} method, is replaced with a \code{Subscriber}, even though it is almost the same\footnote{The \code{onComplete} method in \Cref{lst:pub-sub} does not contain a typing error compared to \Cref{lst:obs-obv}. This is actually how the API is specified!}. It only adds an extra method \code{onSubscribe}, which is called right after the \code{Subscriber} is subscribed to the \code{Publisher}. This \code{onSubscribe} method requires an argument of type \code{Subscription}. This last interface contains two methods: \code{cancel}, which is similar to the \code{unsubscribe} method in \Cref{lst:obs-obv} and a new method \code{request(n: Long)}.

This last method reveals the whole idea behind the Reactive Streams API, namely to let the \code{Subscriber} request a certain amount of elements from the \code{Publisher}. This way the \code{Subscriber} is in charge of how many elements it will receive eventually and the \code{Publisher} just has to send at most this amount of elements by calling the \code{onNext} method.

Taking a step back and evaluating the true meaning of what the Reactive Streams collaboration has come up with, results in the conclusion that this API is interactive rather than reactive, since it lets the consumer (\code{Subscriber}) be in charge of the rate at which data is sent to it. This is discussed in more detail in a conference talk at Lambda Jam 2014 by Erik Meijer \cite{meijer2014-Derivation}. Compared to the earlier discussed \ieb collections, it only adds the features of non-blocking and requesting more than one element (which is what the \ieb interface technically does) per request. We will discuss the consequences of this API in more detail in a later section.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Publisher, Subscriber and Subscription}, label={lst:pub-sub}]
trait Publisher[T] {
    def subscribe(s: Subscriber[T]): Unit
}

trait Subscriber[T] {
    def onNext(t: T): Unit
    def onError(e: Throwable): Unit
    def onComplete(): Unit
    def onSubscribe(s: Subscription): Unit
}

trait Subscription {
    def cancel(): void
    def request(n: Long): Unit
}
\end{lstlisting}
\end{minipage}

\subsection{Reactive pull}
\label{subsec:reactive-pull}
The ideas that sprout from the Reactive Streams initiative have been incorporated in the RxJava library. They kept the original naming conventions of \obs and \obv but added a couple of new methods to the latter: \code{request(n: Int)}, which signals to the \obs that it will be able to handle \code{n} new elements and \code{onStart()}, which performs the initial request from the \obv to the \obs. After the initial request is done, the \obs sends at most \code{n} elements to the \obv, which receives them in the \code{onNext} method. This is now the place where the \obv can call \code{request} again to receive more data.

We recognize these two methods from the Reactive Streams API, where \code{onStart()} was called \code{onSubscribe} and \code{request(n: Long)} was inside the \code{Subscription} interface. Making these slight modifications does not change the intention of the Reactive Streams API: it introduces a feature called \textit{reactive pull} in the \obv to manage the number of elements that are emitted by the upstream \obs.

Earlier versions of RxJava, who did not have this feature only had the ability to communicate upstream by calling the \code{unsubscribe} method. Recall that when an \obv unsubscribes, the \obs is basically signaled to stop emitting any data to that particular \obv. Besides unsubscribing there was no other way in the standard Rx model to communicate upstream.

This feature from Reactive Streams allows the \obv to have some more control by \emph{pulling} from the data source (\obs) at its own pace. RxJava is set up in such a way that processing data under normal circumstances is still push based. Only when the \obv can't handle the speed in which data is sent, it will switch to this pull based model. Wrapping it up in this way, the problem of overproduction is not prevented or gone away but is rather moved up the chain of operators to a point where it can be handled better \cite{RxJava-Wiki-Backpressure}.

With this it limits the number of elements that are in a buffer within certain operators. Using this new feature, the earlier mentioned \code{zip} operator is implemented by using a small buffer for each \obs. It only requests items from one of these sources when there is room for more elements in its buffer. Once all buffers contain at least one element, the operator can remove an element from each buffer, zip these together and push them downstream. After that there is room for at least one extra element in all buffers, hence new requests are sent to the upstream.

Notice that the RxJava wiki \cite{RxJava-Wiki-Backpressure} points out that this method only works when \emph{all} streams that are zipped together respond correctly to the \code{request()} method. This is \emph{not} a requirement for the normal \obs, but it is required for instances of \obs that are used in operators like \code{zip} that depend on reactive pull. RxJava therefore provides operators such as \code{onBackpressureBuffer} and \code{onBackpressureDrop} that respectively buffer and drop data that cannot be consumed immediately by the downstream \obv.
