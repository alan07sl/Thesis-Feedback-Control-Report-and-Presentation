\section{Solutions for overproducing sources discussed}
In the previous chapter we described an interesting problem that occurs when working with streaming data and reactive programming: ``\textit{what happens when the consumer cannot handle the data flow presented by the producer?}''. We also presented a number of solutions, ranging from putting data in an overflow buffer to gaining more control over the \obs. Some of these solutions work perfect under certain conditions, others do not. In this section we will discuss the solutions that were described in section~\ref{sec:fastproc-slowcons} in further detail and reflect on them in the light of the previous section on hot and cold streams.

\subsection{Avoiding overproduction}
As described in section~\ref{subsec:avoiding-overproduction}, \textit{avoiding} is a first line of defense for overproduction. This was done by introducing several operators that either drop some of the data or buffer the data and propagating these buffers downstream for further manipulation. All lossy operators, as well as the \code{buffer} with interval, require a \sch for them to run their interval timers on, hence the output stream of these operators runs on a different thread.

For a hot \obs this kind of defense mechanism is perfect, as the speed at which the source is producing is unknown. This also holds for the cold with-latency \obs, as the production of elements is also bound to a notion of time. For other cold streams, like \code{Observable(1, 2, 3, 4)} or an \obs that is created from a list of elements as shown in \autoref{lst:obs-from-seq}, it does not make any sense to add any overproduction-avoiding operators, as this kind of stream is sequential and will in fact emit at a rate which is determined by the \obv.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Observable from \code{Seq[T]}}, label={lst:obs-from-seq}]
def fromSeq[T](list: Seq[T]): Observable[T] $=$ {
    Observable.create(observer $\Rightarrow$ {
        list.foreach(observer.onNext)
        observer.onCompleted()
    })
}
\end{lstlisting}
\end{minipage}

We are aware of the fact that these operators also have use cases other than avoiding overproduction, such as edge detection or calculating the derivative of a stream of numbers. However, these use cases fall outside the scope of this thesis and for them the comments above do not apply.

\subsection{Callstack blocking}
Subscribing to an \obs is basically nothing more than supplying an \obv to the function within \code{Observable.create} and \emph{sequentially} executing this function using this provided \obv. Once an element is emitted by the source, all operations in the \obs sequence are executed on that element before a second element is emitted. In the sequence of operators in \autoref{lst:operators-obs}, first all operations on $1$ are performed, before $2$ is emitted by the \obs (and then discarded by \code{filter}). This is also the case in \autoref{lst:observeOn} up to line~\ref{line:observeOn-in-observeOn}, after which the elements are scheduled on a different thread and hence are further processed in parallel with emitting new elements from the source.

The order of operations is designed this way to allow for lazy evaluation: never run code that does not need to be run. If a cold \obs contains 5 elements and the operator sequence contains the operator \code{take(2)} (see \autoref{lst:lazy}), only the first 2 elements from this \obs will be evaluated, rather than all 5. After the second element has passed \code{take}, it sends an \code{onCompleted()} downstream, right after the second \code{onNext}, causing the stream to end without evaluating the other 3 elements.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Lazy evaluation}, label={lst:lazy}]
Observable(1, 2, 3, 4, 5)
    // some operators
    .take(2)
    // some more operators
    .subscribe(i $\Rightarrow$ print(i + " ")) // only prints the first and second element
\end{lstlisting}
\end{minipage}

Following this order of operations, we can conclude that basically every higher order function that is executed in the operator sequence is in a sense blocking the callstack during its computation and thereby preventing the source from emitting a next element until the previous element has gone through the whole operator sequence. This is the reason why we concluded in the previous section that avoiding overproduction on a cold no-latency stream does not make any sense. The rate of emission is already determined by the speed in which all operators together are executed.

This is also true for any other kind of stream, whether it is hot or cold, whether or not it has latency and no matter what kind of source emits these elements. This may seem quite surprising, especially for the hot \obs, since we always claimed that this operator is only controlled by its outside environment. However, an example of this is shown in \autoref{lst:blocking-hot-obs}. Here a cold \obs is subscribed to a \subj (line~\ref{line:blocking-hot-subject-subscribe}), making it hot, according to section~\ref{subsec:subjects}. At several points in the operator sequence the time passed since \code{start} is printed and on line~\ref{line:blocking-hot-sleep} the callstack is blocked for 1 second by pausing the thread this whole process runs on. The console output of executing \autoref{lst:blocking-hot-obs} is provided in \autoref{lst:console-output-blocking-hot}\footnote{Due to the inner workings of the JVM, the times shown here may vary by a couple of milliseconds from execution to execution.}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Applying callstack blocking on a hot \obs}, label={lst:blocking-hot-obs}]
def now = System.currentTimeMillis()
val start = now
def timePassed = now - start

val timer = Observable(1, 2, 3, 4)
    .tee(i $\Rightarrow$ println("[" + timePassed + "] emitted - " + i))
val subject = Subject[Int]

subject.tee(i $\Rightarrow$ println("[" + timePassed + "] before - " + i))
    .tee(_ $\Rightarrow$ Thread.sleep(1000)) |\label{line:blocking-hot-sleep}|
    .subscribe(i $\Rightarrow$ println("[" + timePassed + "] after - " + i))
timer.subscribe(subject) |\label{line:blocking-hot-subject-subscribe}|
\end{lstlisting}
\end{minipage}

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=ScalaStyle, caption={Console output from \autoref{lst:blocking-hot-obs}}, label={lst:console-output-blocking-hot}]
[47] emitted - 1
[47] before - 1
[1059] after - 1
[1059] emitted - 2
[1059] before - 2
[2060] after - 2
[2060] emitted - 3
[2060] before - 3
[3062] after - 3
[3062] emitted - 4
[3062] before - 4
[4064] after - 4
\end{lstlisting}
\end{minipage}

From these results it becomes clear the first value was emitted at $t=47\ ms$. Almost immediately after that, the thread on which the whole program is running is blocked for 1 second. At time $t=1059\ ms$ the first value is propagated to the \code{subscribe}. \emph{Only then} the second item is emitted by \code{Observable.apply}.

We can now conclude that even a hot \obs can be controlled by callstack blocking. Notice however that this creates a (potentially ever expanding) buffer of unprocessed \code{onNext} calls within the \code{Observable.create}'s callstack, using an excessive amount of memory. With that we only emulated the naive solution to overproducing streams (see section~\ref{sec:fastproc-slowcons}) by using callstack blocking. The same buffering behavior can also happen within the \code{observeOn} operator, when the other thread used callstack blocking to slow down the stream.

The case described above is one where the thread cannot be blocked safely. It can potentially blow up the program when the buffer gets too big. This is what RxJava warns against in its wiki\cite{RxJava-Wiki-Callstack-Blocking} and what RxMobile warns against in the context of the particular \code{zip} implementation discussed in section~\ref{subsec:callstack-blocking} \cite{RxMobile}. Only certain kinds of operators can use callstack blocking safely, provided with streams that can handle this callstack blocking safely.

Another issue with a hot \obs is what happens when more than one \obv is subscribed and both do callstack blocking. This can potentially lead to even more disastrous situations, where a fast \obv suffers from a slower one and where deadlock situations are inevitable.

In general we can conclude that controlling the flow of data by callstack blocking is already implicitly used in cold no-latency stream, but that it is no good to use callstack blocking on a hot or cold with-latency \obs, unless the developer is completely certain of the stream's behavior.

\subsection{Reactive Streams}
A third way of managing the overflow of data is by using the backpressure technique that was found by the Reactive Streams initiative. The associated API, discussed in section~\ref{subsec:reactive-streams}, consists of a \code{Publisher} that corresponds to the \obs, a \code{Subscriber} that adds an \code{onSubscribe} method to the \obv and a \code{Subscription} with a \code{cancel} and \code{request} method that replaces the Rx \subs. The intention of this API is to let the consumer be in charge rather than the producer. It is the consumer that dictates how many elements it wants to receive and with that it follows the design of the TCP protocol (see section~\ref{subsec:tcp}) as the receiver also sends window size updates (how much more data it is able to handle) to the sender.

The most important question to ask here is whether or not the approach of Reactive Streams is a solution to overproducing sources in reactive programming, as is advertised on the website and implied by the name \cite{Reactive-Streams}. To answer this question, we first need to reflect on the previous paragraph: the consumer is in charge and the producer can at most send the amount of data that is requested by the consumer. Following the definitions from Albert Benveniste and G\'erard Berry \cite{berry1991-Reactive} on reactive and interactive programs (see section~\ref{sec:reactive-programming}), we must conclude that this API creates an \emph{interactive} program, that ``\textit{interacts at its own speed with users or with other programs}'', in this case with the \code{Producer}.

Besides that, we need to take into consideration that `reactive' is the dual of `interactive'. In other words, the interactive interface needs to be dualized in order to become reactive. However, the Reactive Streams API can be constructed from \ieb without using any dualization techniques, as shown by Erik Meijer during a conference talk at Lambda Jam 2014 \cite{meijer2014-Derivation}. Instead he shows that techniques like coproduct, continuation passing style and currying will suffice. This too shows that the Reactive Streams API is not reactive at all, but is interactive instead.

Reactive Streams, even if it is interactive, rather than reactive, still distinguishes itself from the classical set of interactive programming interfaces: \ieb and \ier. Rather than calling \code{moveNext} and \code{current}, a \code{Subscriber} can advertise to its \code{Producer} that it can handle \code{n} more element. It is then up to the latter to send these elements either immediately or after some amount of delay to the former by calling the \code{onNext} method or signal the end of the stream by calling \code{onCompleted} or propagating an error by calling \code{onError}. Just as Rx, the API is set up in such a way that it does not block the program flow, which would be the case when using \ieb and \ier. In that sense, Reactive Streams is definitely not the same as \ieb and \ier, but is rather an API for \textit{asynchronous interactive programming}; it is not just a pull model, it is an \textit{asynchronous} pull model.

The question of whether Reactive Streams is able to solve the regulation of overproducing sources in reactive programming still remains. As we understand now what the API really is (an asynchronous pull model), we can finally reason about what kind of sources it can wrap and with that replace the Rx API.

Both a cold no-latency source and a cold with-latency source are perfect candidates to be wrapped in this kind of stream as they are already interactive by themselves. The \code{Subscriber} can pull as much data from the source as it wants, only restricted by the size of the source, in case it is finite. Note that for the no-latency source the response of a request will be immediate, whereas the with-latency source will take some amount of time to produce its next \code{n} values.

A hot source on the other hand is not as suitable for the Reactive Streams API as the cold sources are. There is no way for the \code{Producer} in which the hot source is wrapped to interact with it. It cannot request the next \code{n} elements from it, nor can it request to slow down or speed up. This is implied in the nature of a hot source. The only way to have a hot source be wrapped in a \code{Producer} is to drain its the data in a buffer or queue and sending the requested amount of elements from this buffer to the \code{Subscriber}. In general this is again that dangerous move of possibly storing unwieldy amounts of data for later processing as is described in earlier sections. That being said, in special cases, where in the long term the downstream can empty the buffer before it starts overflowing, it is in fact possible to use this API for it. An example of this may be a source with bursty behavior, where in a short amount of time a large amount of data is produced, after which no data is produced for a longer amount of time.

\subsection{RxJava and reactive pull}
RxJava started of as a port of Rx.Net for the JVM and was until version 0.20 purely reactive. It did not have any other policies for handling backpressure than the ones discussed in section~\ref{subsec:avoiding-overproduction}. In version 0.20 backpressure support was introduced as a result of collaborations with the Reactive Streams initiative. This implementation (also referred to as \textit{reactive pull}) did not change the original Rx interfaces but rather added extra methods that were taken from the Reactive Streams API. Due to these changes, the reactiveness is partially lost, as the data flow was now controlled by \code{request(n)} calls from the \obv.

From version 0.20 onward, RxJava has a lot of similarities with the Reactive Streams API in terms of handling backpressure. It is therefore well equipped to handle cold sources by wrapping them in the \obs. However, in contrast to Reactive Streams, this API is still able to correctly coop with hot sources. The RxJava wiki states that for this the \obs needs to be initialized with \code{request(Long.MAX\_VALUE)}, which orders the \obs to emit data at its own pace \cite{RxJava-Wiki-Backpressure}.

























